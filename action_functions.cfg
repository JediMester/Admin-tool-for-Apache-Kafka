#!/bin/bash

### Action függvények

#declare -a sh_files=()

declare -a topics=()

function make_pwd_and_get_controller() {
	
	who_am_i="$(who am i | awk '{print $1}')"
	
	pw_dir="/home/kafka/pwd/"
	
	if [[ ! -d $pw_dir ]]; then
		mkdir -p $pw_dir
	fi
	
	pwd_file="$pw_dir$who_am_i.passwd"
	
	if [[ ! -f $pwd_file ]]; then
		touch $pwd_file
		echo "${YEL}IPA jelszó (lefutás után a pw fájl törlésre kerül):${NC} "
		read -s pwd && echo $pwd >> $pwd_file
		chmod +rwx $pwd_file
		#chown $who_am_i:$who_am_i $pwd_file
	fi
	
	dev_hosts=(srv01.dc02.dev.kafka srv02.dc02.dev.kafka srv03.dc02.dev.kafka srv04.dc02.dev.kafka srv05.dc02.dev.kafka srv06.dc02.dev.kafka)
	uat_hosts=(srv01.dc01.uat.kafka srv02.dc01.uat.kafka srv03.dc01.uat.kafka srv04.dc01.uat.kafka srv05.dc01.uat.kafka srv06.dc01.uat.kafka)
	pp_hosts=(srv01.dc01.preprod.kafka srv02.dc01.preprod.kafka srv03.dc01.preprod.kafka srv04.dc01.preprod.kafka srv01.dc02.preprod.kafka srv02.dc02.preprod.kafka srv03.dc02.preprod.kafka srv04.dc02.preprod.kafka)
	prod_hosts=(srv01.dc01.prod.kafka srv02.dc01.prod.kafka srv03.dc01.prod.kafka srv04.dc01.prod.kafka srv05.dc01.prod.kafka srv06.dc01.prod.kafka srv07.dc01.prod.kafka srv08.dc01.prod.kafka srv01.dc02.prod.kafka srv02.dc02.prod.kafka srv03.dc02.prod.kafka srv04.dc02.prod.kafka srv05.dc02.prod.kafka srv06.dc02.prod.kafka srv07.dc02.prod.kafka srv08.dc02.prod.kafka)

	get_env
	
	if [[ "$env" == "ua" ]]; then
		kaf_broker_list="${uat_hosts[@]}"
	elif [[ "$env" == "dv" ]]; then
		kaf_broker_list="${dev_hosts[@]}"
	elif [[ "$env" == "pp" ]]; then
		kaf_broker_list="${pp_hosts[@]}"
	else
		kaf_broker_list="${prod_hosts[@]}"
	fi
	
	choose_zookeeper
	controller_broker=$($zookeeper_shell $selected_zookeeper get /brokers/ids/$(/opt/kafka/apache-kafka/bin/zookeeper-shell.sh $selected_zookeeper get /controller | tail -1 | jq .brokerid) | tail -1 | jq .endpoints[])
	if [[ "$env" == "pp" ]]; then
		controller_node=$(echo $controller_broker | grep "SASL_SSL" | sed 's/.*\///' | awk -F '"' {'print $1'} | cut -c1-35)
	else
		controller_node=$(echo $controller_broker | grep "SASL_SSL" | sed 's/.*\///' | awk -F '"' {'print $1'} | cut -c1-31)
	fi
	
}

function rolling_restart() {
	
	make_pwd_and_get_controller
	
	enc_pwd_file=$(find $pw_dir -type f -iwholename $pwd_file.enc)
	
	if [[ -f $enc_pwd_file ]]; then
		echo ""
		echo "${YEL}IPA jelszó beolvasása az SHA256 passwd decrypt-áláshoz${NC}"
		#openssl enc -d -aes-256-cbc -in $enc_pwd_file -out $pwd_file -kfile $pwd_file
		openssl enc -d -salt -pbkdf2 -in $enc_pwd_file -out $pwd_file -kfile $pwd_file
	fi

    # URP csekkolás pssh után
    #under_rep_checker
  
	if [[ ! -f $enc_pwd_file ]]; then
		echo ""
		echo "${YEL}IPA jelszó beolvasása az SHA256 passwd encrypt-áláshoz.${NC}"
		#read -s pwd
		#openssl enc -e -aes-256-cbc -salt -in $pwd_file -out $enc_pwd_file -kfile $pwd_file
		openssl enc -e -salt -pbkdf2 -in $pwd_file -out $enc_pwd_file -kfile $pwd_file
	fi
	
	#if [[ ${env} == "pp" ]]; then
	#	controller_node=${controller_node}
	#	
	#	kaf_broker_list=("${kaf_broker_list[@]/$controller_node}")
	#	
	#	kaf_broker_list+=("$controller_node")
	#else
	#	controller_host=$(eval echo \${broker_list_${env}}$controller_node)
	#	
	#	kaf_broker_list=("${kaf_broker_list[@]/$controller_host}")
	#
	#	kaf_broker_list+=("$controller_host")
	#fi
	
	controller_host=$(eval echo \${broker_list_${env}}$controller_node)
		
	kaf_broker_list=("${kaf_broker_list[@]/$controller_host}")
	
	kaf_broker_list+=("$controller_host")
	
	under_rep_partitions=0
	under_rep_checker
	echo "${LBL}Rolling restart indul - ${NC}"$(date +%Y-%m-%d_%H-%M-%S)
	#get_controller_broker
	case ${env} in
	pp)
	while : ; do
		for broker in ${kaf_broker_list[@]}
			do
				broker_host=$(eval echo \${broker_list_${env}}$broker)
				controller_host=$(eval echo \${broker_list_${env}}$controller_node)
				
				#get_broker_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "systemctl status kafka | grep -i active | awk '{print \$2 \$3}' | tr -d '\n\t\r '")
				get_broker_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} "systemctl status kafka | grep -i active | awk '{print \$2 \$3}' | tr -d '\n\t\r '")
				#if [[ "${broker}" != "${controller_node}" ]]; then
				if [[ "${broker_host}" != "${controller_host}" ]]; then
					#echo "${LBL}URP szám:${NC} ${YEL}${under_rep_partitions}${NC}"
					echo "${LBL}Restart indul:${NC} ${YEL}$broker${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
					echo ""
					sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} -t "sudo su - kafka; sudo systemctl restart kafka"
					echo "Várunk ${YEL}15 sec-et${NC}, hogy a restart befejeződjön..."
					echo ""
					countdown 15
					echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
					echo ""
						if [[ ${get_broker_status} != "active(running)" ]]; then
							echo "Várunk még ${YEL}10 sec-et${NC}, hogy a restart biztosan befejeződjön..."
							echo ""
							countdown 10
							echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
						else
							echo "${LG}A restart sikeresen befejeződött.${NC}"
						fi
						
					if { [[ ${get_broker_status} != "active(running)" ]] && [[ ${under_rep_partitions} == 0 ]] ;}; then
						echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
						echo ""
						echo "${LBL}Restart indul:${NC} ${YEL}$broker_host${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
						echo ""
						sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} -t "sudo su - kafka; sudo systemctl restart kafka"
						echo "Várunk ${YEL}15 sec-et${NC}, hogy a restart befejeződjön..."
						countdown 15
						echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
					else
						if [[ ${under_rep_partitions} != 0 ]]; then
							break
						fi
						
						under_rep_checker
						
						count=0
						while sleep 30; do
							under_rep_checker
							echo "Várunk ${YEL}30 sec-et${NC} a következő URP check-ig..."
							countdown 30
							if [[ ${under_rep_partitions} == 0 ]]; then
								break
							fi
						done
					fi
				
				else
					echo "${LBL}Kontroller node újraindul:${NC} ${YEL}${controller_host}${NC}"
					#get_controller_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${controller_host} "systemctl status kafka | grep 'Active:' | awk '{print \$2 \$3}' | tr -d '\n\t\r '")
					controller_host=$(eval echo \${broker_list_${env}}$controller_node)
					get_controller_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${controller_host} "systemctl status kafka | grep -i active | awk '{print \$2 \$3}'")
					if [[ "${broker_host}" == "${controller_host}" ]]; then
						under_rep_checker
							if [[ ${under_rep_partitions} == 0 ]]; then
								sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} -t "sudo su - kafka; sudo systemctl restart kafka"
								echo "${LBL}Service státusz:${NC} ${LG}$get_controller_status${NC}"
								echo ""
									if [[ ${get_controller_status} != "active(running)" ]]; then
										echo "Várunk még ${YEL}10 sec-et${NC}, hogy a restart biztosan befejeződjön..."
										#sleep 10
										echo ""
										countdown 10
									else
										echo "${LG}A restart sikeresen befejeződött. - ${NC}"$(date +%Y-%m-%d_%H-%M-%S)
									fi
							elif [[ ${under_rep_partitions} != 0 ]]; then
								
								break
															
								under_rep_checker
									
								count=0
								while sleep 30; do
									under_rep_checker
									echo "Várunk ${YEL}30 sec-et${NC} a következő URP check-ig..."
									countdown 30
									if [[ ${under_rep_partitions} == 0 ]]; then
										break
										echo "${LBL}Restart indul:${NC} ${YEL}$broker_host${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
										echo ""
										sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} -t "sudo su - kafka; sudo systemctl restart kafka"
									fi
								done
							else
								echo "${LG}Nincs URP, a restart sikeresen befejeződött!{NC} - "$(date +%Y-%m-%d_%H-%M-%S)
								
							fi
					fi
				fi
				
		done
		
		break
		#continue
	
	done
	;;
				
	*)
	while : ; do
		for broker in ${kaf_broker_list[@]}
			do
				broker_host=$(eval echo \${broker_list_${env}}$broker)
				controller_host=$(eval echo \${broker_list_${env}}$controller_node)
				
				get_broker_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} "systemctl status kafka | grep -i active | awk '{print \$2 \$3}' | tr -d '\n\t\r '")
				if [[ "${broker_host}" != "${controller_host}" ]]; then
					#echo "${LBL}URP szám:${NC} ${YEL}${under_rep_partitions}${NC}"
					echo "${LBL}Restart indul:${NC} ${YEL}$broker_host${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
					echo ""
					sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} "~/bin/asroot systemctl restart kafka"
					echo "Várunk ${YEL}15 sec-et${NC}, hogy a restart befejeződjön..."
					echo ""
					countdown 15
					echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
					echo ""
						if [[ ${get_broker_status} != "active(running)" ]]; then
							echo "Várunk még ${YEL}10 sec-et${NC}, hogy a restart biztosan befejeződjön..."
							echo ""
							countdown 10
							echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
						else
							echo "${LG}A restart sikeresen befejeződött.${NC}"
						fi
						
					if { [[ ${get_broker_status} != "active(running)" ]] && [[ ${under_rep_partitions} == 0 ]] ;}; then
						echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
						echo ""
						echo "${LBL}Restart indul:${NC} ${YEL}$broker_host${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
						echo ""
						sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} "~/bin/asroot systemctl restart kafka"
						echo "Várunk ${YEL}15 sec-et${NC}, hogy a restart befejeződjön..."
						countdown 15
						echo "${LBL}Service státusz:${NC} ${LG}$get_broker_status${NC}"
					else
						if [[ ${under_rep_partitions} != 0 ]]; then
							break
						fi
						
						under_rep_checker
						
						count=0
						while sleep 30; do
							under_rep_checker
							echo "Várunk ${YEL}30 sec-et${NC} a következő URP check-ig..."
							countdown 30
							if [[ ${under_rep_partitions} == 0 ]]; then
								break
								#echo "${LBL}Restart indul:${NC} ${YEL}$broker_host${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
								#echo ""
								#sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} "~/bin/asroot systemctl restart kafka"
							fi
						done
					fi
				 
				else
					echo "${LBL}Kontroller node újraindul:${NC} ${YEL}${controller_host}${NC}"
					#get_controller_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${controller_host} "systemctl status kafka | grep 'Active:' | awk '{print \$2 \$3}' | tr -d '\n\t\r '")
					controller_host=$(eval echo \${broker_list_${env}}$controller_node)
					get_controller_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${controller_host} "systemctl status kafka | grep -i active | awk '{print \$2 \$3}'")
					if [[ "${broker_host}" == "${controller_host}" ]]; then
						under_rep_checker
							if [[ ${under_rep_partitions} == 0 ]]; then
								sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} "~/bin/asroot systemctl restart kafka"
								echo "${LBL}Service státusz:${NC} ${LG}$get_controller_status${NC}"
								echo ""
									if [[ ${get_controller_status} != "active(running)" ]]; then
										echo "Várunk még ${YEL}10 sec-et${NC}, hogy a restart biztosan befejeződjön..."
										#sleep 10
										echo ""
										countdown 10
									else
										echo "${LG}A restart sikeresen befejeződött. - ${NC}"$(date +%Y-%m-%d_%H-%M-%S)
									fi
							elif [[ ${under_rep_partitions} != 0 ]]; then
								
								break
															
								under_rep_checker
									
								count=0
								while sleep 30; do
									under_rep_checker
									echo "Várunk ${YEL}30 sec-et${NC} a következő URP check-ig..."
									countdown 30
									if [[ ${under_rep_partitions} == 0 ]]; then
										break
										echo "${LBL}Restart indul:${NC} ${YEL}$broker_host${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
										echo ""
										sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker_host} "~/bin/asroot systemctl restart kafka"
									fi
								done
							else
								echo "${LG}Nincs URP, a restart sikeresen befejeződött!{NC} - "$(date +%Y-%m-%d_%H-%M-%S)
								
							fi
					fi
							
				fi
		done
		
		break
		#continue
	
	done	
	;;
	esac
					
		#done
		#
		#break
		##continue
	
	#done
	
	sleep 1.5
	echo "${GRE}A rolling restart sikeresen befejeződött!${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
	
	rm -rfv ${pwd_file}
	echo "${RED}Jelszó fájl törölve!${NC}"
	

}

function stop_and_disable_kaf() {
	
	who_am_i="$(who am i | awk '{print $1}')"
	
	pw_dir="/home/kafka/pwd/"
	
	if [[ ! -d $pw_dir ]]; then
		mkdir -p $pw_dir
	fi
	
	pwd_file="$pw_dir$who_am_i.passwd"
	
	if [[ ! -f $pwd_file ]]; then
		touch $pwd_file
		echo "IPA jelszó (rolling restart után a pw fájl törlésre kerül): "
		read -s pwd && echo $pwd >> $pwd_file
		chmod +rwx $pwd_file
		#chown $who_am_i:$who_am_i $pwd_file
	fi
	
	dev_hosts=(srv01.dc02.dev.kafka srv02.dc02.dev.kafka srv03.dc02.dev.kafka srv04.dc02.dev.kafka srv05.dc02.dev.kafka srv06.dc02.dev.kafka)
	uat_hosts=(srv01.dc01.uat.kafka srv02.dc01.uat.kafka srv03.dc01.uat.kafka srv04.dc01.uat.kafka srv05.dc01.uat.kafka srv06.dc01.uat.kafka)
	#pp_hosts=(srv01.dc01.preprod.kafka srv02.dc01.preprod.kafka srv03.dc01.preprod.kafka srv04.dc01.preprod.kafka srv01.dc02.preprod.kafka srv02.dc02.preprod.kafka srv03.dc02.preprod.kafka srv04.dc02.preprod.kafka)
	pp_dc01=(srv01.dc01.preprod.kafka srv02.dc01.preprod.kafka srv03.dc01.preprod.kafka srv04.dc01.preprod.kafka)
	pp_dc02=(srv01.dc02.preprod.kafka srv02.dc02.preprod.kafka srv03.dc02.preprod.kafka srv04.dc02.preprod.kafka)
	#prod_hosts=(srv01.dc01.prod.kafka srv02.dc01.prod.kafka srv03.dc01.prod.kafka srv04.dc01.prod.kafka srv05.dc01.prod.kafka srv06.dc01.prod.kafka srv07.dc01.prod.kafka srv08.dc01.prod.kafka srv01.dc02.prod.kafka srv02.dc02.prod.kafka srv03.dc02.prod.kafka srv04.dc02.prod.kafka srv05.dc02.prod.kafka srv06.dc02.prod.kafka srv07.dc02.prod.kafka srv08.dc02.prod.kafka)
	prod_dc01=(srv01.dc01.prod.kafka srv02.dc01.prod.kafka srv03.dc01.prod.kafka srv04.dc01.prod.kafka srv05.dc01.prod.kafka srv06.dc01.prod.kafka srv07.dc01.prod.kafka srv08.dc01.prod.kafka)
	prod_dc02=(srv01.dc02.prod.kafka srv02.dc02.prod.kafka srv03.dc02.prod.kafka srv04.dc02.prod.kafka srv05.dc02.prod.kafka srv06.dc02.prod.kafka srv07.dc02.prod.kafka srv08.dc02.prod.kafka)
	
	#get_env
	
	if [[ "$env" == "ua" ]]; then
		kaf_broker_list="${uat_hosts[@]}"
	elif [[ "$env" == "dv" ]]; then
		kaf_broker_list="${dev_hosts[@]}"
	elif [[ "$env" == "pp" ]]; then
		case ${dc_option} in
		1)
			kaf_broker_list="${pp_dc01[@]}"
			;;
		2)
			kaf_broker_list="${pp_dc02[@]}"
			;;
		esac
	else
		case ${dc_option} in
		1)
			kaf_broker_list="${prod_dc01[@]}"
			;;
		2)
			kaf_broker_list="${prod_dc02[@]}"
			;;
		esac
	fi
	
	for broker in ${kaf_broker_list[@]}; do
		get_broker_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "systemctl status kafka | grep -i active | awk '{print \$2 \$3}' | tr -d '\n\t\r '")
		echo "${LBL}Leállítás és service disable indul:${NC} ${YEL}$broker${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
		echo ""
		sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "~/bin/asroot systemctl stop kafka"
		countdown 30
		sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "~/bin/asroot systemctl disable kafka"
		sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "~/bin/asroot systemctl daemon-reload"
		echo "${GRE}Service státusz:${NC} ${YEL}$get_broker_status${NC}"
		echo ""
	done
	
	echo "${GRE}Service leállítás sikeresen befejeződött!{NC} - "$(date +%Y-%m-%d_%H-%M-%S)
	
	rm -rf ${pwd_file}
	echo "${RED}Jelszó fájl törölve!${NC}"
	
}

function start_and_enable_kaf() {
	
	who_am_i="$(who am i | awk '{print $1}')"
	
	pw_dir="/home/kafka/pwd/"
	
	if [[ ! -d $pw_dir ]]; then
		mkdir -p $pw_dir
	fi
	
	pwd_file="$pw_dir$who_am_i.passwd"
	
	if [[ ! -f $pwd_file ]]; then
		touch $pwd_file
		echo "IPA jelszó (rolling restart után a pw fájl törlésre kerül): "
		read -s pwd && echo $pwd >> $pwd_file
		chmod +rwx $pwd_file
		#chown $who_am_i:$who_am_i $pwd_file
	fi
	
	dev_hosts=(srv01.dc02.dev.kafka srv02.dc02.dev.kafka srv03.dc02.dev.kafka srv04.dc02.dev.kafka srv05.dc02.dev.kafka srv06.dc02.dev.kafka)
	uat_hosts=(srv01.dc01.uat.kafka srv02.dc01.uat.kafka srv03.dc01.uat.kafka srv04.dc01.uat.kafka srv05.dc01.uat.kafka srv06.dc01.uat.kafka)
	#pp_hosts=(srv01.dc01.preprod.kafka srv02.dc01.preprod.kafka srv03.dc01.preprod.kafka srv04.dc01.preprod.kafka srv01.dc02.preprod.kafka srv02.dc02.preprod.kafka srv03.dc02.preprod.kafka srv04.dc02.preprod.kafka)
	pp_dc01=(srv01.dc01.preprod.kafka srv02.dc01.preprod.kafka srv03.dc01.preprod.kafka srv04.dc01.preprod.kafka)
	pp_dc02=(srv01.dc02.preprod.kafka srv02.dc02.preprod.kafka srv03.dc02.preprod.kafka srv04.dc02.preprod.kafka)
	#prod_hosts=(srv01.dc01.prod.kafka srv02.dc01.prod.kafka srv03.dc01.prod.kafka srv04.dc01.prod.kafka srv05.dc01.prod.kafka srv06.dc01.prod.kafka srv07.dc01.prod.kafka srv08.dc01.prod.kafka srv01.dc02.prod.kafka srv02.dc02.prod.kafka srv03.dc02.prod.kafka srv04.dc02.prod.kafka srv05.dc02.prod.kafka srv06.dc02.prod.kafka srv07.dc02.prod.kafka srv08.dc02.prod.kafka)
	prod_dc01=(srv01.dc01.prod.kafka srv02.dc01.prod.kafka srv03.dc01.prod.kafka srv04.dc01.prod.kafka srv05.dc01.prod.kafka srv06.dc01.prod.kafka srv07.dc01.prod.kafka srv08.dc01.prod.kafka)
	prod_dc02=(srv01.dc02.prod.kafka srv02.dc02.prod.kafka srv03.dc02.prod.kafka srv04.dc02.prod.kafka srv05.dc02.prod.kafka srv06.dc02.prod.kafka srv07.dc02.prod.kafka srv08.dc02.prod.kafka)
	
	#get_env
	
	if [[ "$env" == "ua" ]]; then
		kaf_broker_list="${uat_hosts[@]}"
	elif [[ "$env" == "dv" ]]; then
		kaf_broker_list="${dev_hosts[@]}"
	elif [[ "$env" == "pp" ]]; then
		case ${dc_option} in
		1)
			kaf_broker_list="${pp_dc01[@]}"
			;;
		2)
			kaf_broker_list="${pp_dc02[@]}"
			;;
		esac
	else
		case ${dc_option} in
		1)
			kaf_broker_list="${prod_dc01[@]}"
			;;
		2)
			kaf_broker_list="${prod_dc02[@]}"
			;;
		esac
	fi
	
	for broker in ${kaf_broker_list[@]}; do
		get_broker_status=$(sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "systemctl status kafka | grep -i active | awk '{print \$2 \$3}' | tr -d '\n\t\r '")
		echo "${LBL}Indítás és service enable indul:${NC} ${YEL}$broker${NC} - "$(date +%Y-%m-%d_%H-%M-%S)
		echo ""
		sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "~/bin/asroot systemctl enable kafka"
		sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "~/bin/asroot systemctl daemon-reload"
		sshpass -f $pwd_file ssh -o stricthostkeychecking=no -A ${who_am_i}@${broker} "~/bin/asroot systemctl start kafka"
		countdown 30
		echo "${GRE}Service státusz:${NC} ${YEL}$get_broker_status${NC}"
		echo ""
	done
	
	echo "${GRE}Service indítás sikeresen befejeződött!{NC} - "$(date +%Y-%m-%d_%H-%M-%S)
	
	rm -rf ${pwd_file}
	echo "${RED}Jelszó fájl törölve!${NC}"

}

function describe_topic() {
	
	topics+="$topic"
	for topic in "${topics[@]}";
		do
			#desc_topic_cmd="$kafka_consumer_gr_path --bootstrap-server $selected_broker --command-config $consumer_properties --describe --topic $topic --all-groups"
			desc_topic_cmd="$kafka_topics_path --bootstrap-server=$selected_broker --describe --topic $topic --command-config $consumer_properties"
			echo "$desc_topic_cmd" >> "$sh_file"
		done
	
	cat "$sh_file"
}

function get_user_acl() {
	
	user_acl_cmd="$kafka_acls_path --bootstrap-server=$selected_broker --command-config $command_config_properties --list --principal User:$user"
	topic_acl_cmd="$kafka_acls_path --bootstrap-server=$selected_broker --command-config $command_config_properties --list --topic $topic"
	
	if [[ $user_or_topic == "1" ]]; then
		echo "$user_acl_cmd" >> "$sh_file"
		cat "$sh_file"
		
	else
		echo "$topic_acl_cmd" >> "$sh_file"
		cat "$sh_file"
		
	fi

}

function get_acl_complex() {
	user_acl_cmd="$kafka_acls_path --bootstrap-server=$selected_broker --command-config=$command_config_properties --list --principal User:$user"
	topic_acl_cmd="$kafka_acls_path --bootstrap-server=$selected_broker --command-config=$command_config_properties --list --topic $topic"
	
	if [[ $user_or_topic == "1" ]]; then
		#echo "$user_acl_cmd" >> "$acl_file"
		$user_acl_cmd
	else
		#echo "$topic_acl_cmd" >> "$acl_file"
		$topic_acl_cmd
	fi
}

function consume_message() {
	
	consume_command="$kafka_console_consumer --bootstrap-server $selected_broker --topic $topic --from-beginning >> /home/kafka/$message_txt"
	echo "$consume_command" >> "$sh_file"
	cat "$sh_file"

}

function produce_message() {
	
	produce_command="$kafka_console_producer --bootstrap-server $selected_broker --producer.config=$producer_properties --topic $topic < /home/kafka/$message_txt"
	echo "$produce_command" >> "$sh_file"
	cat "$sh_file"
}

function last_offset_per_partition() {

	last_offset_command="$kafka_run_class kafka.tools.GetOffsetShell --broker-list $selected_broker --command-config $command_config_properties --topic $topic | awk -F  ":" '{sum += $3} END {print \"Result: \"sum}'"
	echo "$last_offset_command" >> "$sh_file"
	cat "$sh_file"
}

function create_topic() {
	
	#sh_files+=(""$sh_file"")
	
	local zk_port=2181
	
	if [[ "$kafka_version" == "2.7.0" ]]; then
		echo "$kafka_topics_path --create --topic $topic --zookeeper $selected_zookeeper:$zk_port --partitions $partitions --replication-factor $replication_factor" #>> "$sh_file"
		#echo "$kafka_topics_path --create --topic $topic --zookeeper $selected_zookeeper --partitions $partitions --replication-factor $replication_factor" #>> "$sh_file"
		
	else
		echo "$kafka_topics_path --create --bootstrap-server $selected_broker --command-config $consumer_properties  --topic $topic --partitions $partitions --replication-factor $replication_factor" #>> "$sh_file"
		
	fi
	
}	

function create_topic_simple_ot() {	
	
	#sh_files+=(""$sh_file"")
	
	if [[ ${LOGGING} == "true" ]]; then
		echo "$kafka_topics_path --create --topic $topic --zookeeper $selected_zookeeper:$zk_port --partitions $partitions --replication-factor $replication_factor" #>> "$sh_file"
		
	else
		echo "$kafka_topics_path --create --topic $topic --zookeeper $selected_zookeeper:$zk_port --partitions $partitions --replication-factor $replication_factor" #>> "$sh_file"
	
	fi
}

#function create_topic() {
#	if { ( [[ "$there_is_csv" == "yes" ]] && [[ ${csv_file_type} == "complex" ]] && [[ "$action" == "1" ]] ) || \
#		 ( [[ "$there_is_csv" == "yes" ]] && [[ ${csv_file_type} == "simple_only_topics" ]] && [[ "$action" == "4" ]] ) || \
#		 ( [[ "$there_is_csv" == "no" ]] && [[ "$action" == "1" ]] ) && \
#		 [[ ! -z "$topic" ]] && [[ ! -z "$replication_factor" ]] && [[ ! -z "$partitions" ]] ;}
#	then
#		if [[ "${LOGGING}" == "true" ]]; then
#			#if [[ "$kafka_version" == "2.7.0" ]]; then
#				"$kafka_topics_path" --create --topic "$topic" --zookeeper "$selected_zookeeper":"$zk_port" --partitions "$partitions" --replication-factor "$replication_factor"
#				
#				log_mgmt "$ticket_num"
#				
#				log_zipper
#			#else
#			#	#"$kafka_topics_path" --create --bootstrap-server "$selected_broker":"$bts_port" --command-config "$consumer_properties" --partitions "$partitions" --replication-factor "$replication_factor" --topic "$topic"
#			#	"$kafka_topics_path" --create --bootstrap-server "$selected_broker" --command-config "$consumer_properties"  --topic "$topic" --partitions "$partitions" --replication-factor "$replication_factor"
#			#	
#			#	log_mgmt "$ticket_num"
#			#	
#			#	log_zipper
#			#fi
#		else
#			#if [[ "$kafka_version" == "2.7.0" ]]; then
#				"$kafka_topics_path" --create --topic "$topic" --zookeeper "$selected_zookeeper":"$zk_port" --partitions "$partitions" --replication-factor "$replication_factor"
#				
#			#else
#			#	#"$kafka_topics_path" --create --bootstrap-server "$selected_broker":"$bts_port" --command-config "$consumer_properties" --partitions "$partitions" --replication-factor "$replication_factor" --topic "$topic"
#			#	"$kafka_topics_path" --create --bootstrap-server "$selected_broker" --command-config "$consumer_properties"  --topic "$topic" --partitions "$partitions" --replication-factor "$replication_factor"
#			#fi
#			
#		fi
#		
#	else
#		local missing_vars=()
#		[ -z "$topic" ] && missing_vars=+"$topic"
#		[ -z "$replication_factor" ] && missing_vars=+"$replication_factor"
#		[ -z "$partitions" ] && missing_vars=+"$partitions"
#		
#		check_error "$kafka_topics_path --create" "${missing_vars[@]}"
#	fi	
#}

function create_topic_with_retention() {
	
	#sh_files+=(""$sh_file"")
	
	local zk_port=2181

	if [[ "$kafka_version" == "2.7.0" ]]; then
		echo "$kafka_topics_path --create --zookeeper $selected_zookeeper:$zk_port --create --topic $topic --config retention.ms=$day_to_ms_retention --partitions $partitions --replication-factor $replication_factor" #>> "$sh_file"
		
	else
		echo "$kafka_topics_path --create --bootstrap-server $selected_broker --command-config $consumer_properties  --topic $topic --partitions $partitions --replication-factor $replication_factor  --config retention.ms=$day_to_ms_retention" #>> "$sh_file"
		
	fi
}

function create_consumer_complex_csv() {
	
	#sh_files+=(""$sh_file"")
	
	if [[ "$action" == "3" ]]; then
		if [[ "$kafka_version" == "2.7.0" ]]; then
			echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --consumer --allow-principal User:$user --topic $topic_read --group $group_id" #>> "$sh_file"	
			
		else
			echo "$kafka_acls_path --bootstrap-server $selected_broker --add --consumer --allow-principal User:$user --topic $topic_read --group $group_id --command-config $consumer_properties" #>> "$sh_file"
			
		fi
	
	elif [[ "$action" == "4" ]]; then
		if [[ "$kafka_version" == "2.7.0" ]]; then
			echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --consumer --allow-principal User:$user --topic $topic_read --group $group_id_prefix-" #>> "$sh_file"
			
		else
			echo "$kafka_acls_path --bootstrap-server $selected_broker --add --consumer --allow-principal User:$user --topic $topic_read --group $group_id_prefix- --command-config $consumer_properties" #>> "$sh_file"
			
		fi
		
	fi
}

function create_consumer() {
	# Experimental shit (error kiíratás)
	#local actual_cmd=""
	
	#sh_files+=(""$sh_file"")
	
	if { [[ "$there_is_csv" == "no" ]] && [[ "$action" == "3" ]] ;}; then
		if [[ "$is_prefixed" == "no" ]]; then
			#if [[ "$kafka_version" == "2.7.0" ]]; then
				echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --consumer --allow-principal User:$user --topic $topic --group $consumer_group" #>> "$sh_file"
				
			#else
			#	"$kafka_acls_path" --bootstrap-server "$selected_broker" --command-config "$consumer_properties" --add --consumer --allow-principal User:"$user" --topic "$topic" --group "$consumer_group" --command-config "$consumer_properties"
			#
			#fi	
		elif [[ "$is_prefixed" == "yes" ]]; then
			#if [[ "$kafka_version" == "2.7.0" ]]; then
				echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --consumer --allow-principal User:$user --topic $topic --group $consumer_group-" #>> "$sh_file"
			
			#else
			#	"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --consumer --allow-principal User:"$user" --topic "$topic" --group "$consumer_group-" --command-config "$consumer_properties"
			#
			#fi
		fi
		
	elif { [[ "$there_is_csv" == "yes" ]] && [[ "${csv_file_type}" == "simple_only_topics" ]] && [[ "$action" == "6" ]] ;}; then
		if [[ "$is_prefixed" == "no" ]]; then
			#if [[ "$kafka_version" == "2.7.0" ]]; then
				echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --consumer --allow-principal User:$user --topic $topic --group $consumer_group" #>> "$sh_file"
				
			#else
			#	"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --consumer --allow-principal User:"$user" --topic "$topic" --group "$consumer_group" --command-config "$consumer_properties"
			#
			#fi
		elif [[ "$is_prefixed" == "yes" ]]; then
			#if [[ "$kafka_version" == "2.7.0" ]]; then
				echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --consumer --allow-principal User:$user --topic $topic --group $consumer_group-" #>> "$sh_file"
				
			#else
			#	"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --consumer --allow-principal User:"$user" --topic "$topic" --group "$consumer_group-" --command-config "$consumer_properties"
			#
			#fi
		fi
			
	else
		local missing_vars=()
		[ -z "$user" ] && missing_vars=+"$user"
		[ -z "$topic" ] && missing_vars=+"$topic"
		[ -z "$consumer_group" ] && missing_vars=+"$consumer_group"
		
		check_error "$kafka_acls_path --add consumer" "${missing_vars[@]}"
		
	fi
	
	
	
}

function create_producer_complex_csv() {
	
	#sh_files+=("$sh_file")
	
	if [[ "$action" == "5" ]]; then
		if [[ "$kafka_version" == "2.7.0" ]]; then
			echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --producer --allow-principal User:$user --topic $topic_write" #>> "$sh_file"
			
		else
			echo "$kafka_acls_path --bootstrap-server $selected_broker --add --producer --allow-principal User:$user --topic $topic_write --command-config $producer_properties" #>> "$sh_file"
			
		fi
	
	elif [[ "$action" == "6" ]]; then
		if [[ "$kafka_version" == "2.7.0" ]]; then
			echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --producer --allow-principal User:$user --topic $topic_write --resource-pattern-type PREFIXED" #>> "$sh_file"
			
		else
			echo "$kafka_acls_path --bootstrap-server $selected_broker --add --producer --allow-principal User:$user --topic $topic_write --resource-pattern-type PREFIXED --command-config $producer_properties" #>> "$sh_file"
			
		fi
		
	fi
}

function create_producer() {
	
	#sh_files+=("$sh_file")
	
	if [[ "$there_is_csv" == "no" ]]; then
		if [[ "$action" == "4" ]]; then
			if [[ "$kafka_version" == "2.7.0" ]]; then
				"$kafka_acls_path" --authorizer-properties zookeeper.connect="$selected_zookeeper" --add --producer --allow-principal User:"$user" --topic "$topic"
			
			else
				"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --producer --allow-principal User:"$user" --topic "$topic" --command-config "$producer_properties"	
			
			fi
			
		elif [[ "$action" == "5" ]]; then
			if [[ "$kafka_version" == "2.7.0" ]]; then
				"$kafka_acls_path" --authorizer-properties zookeeper.connect="$selected_zookeeper" --add --producer --allow-principal User:"$user" --topic "$topic" --resource-pattern-type PREFIXED
				
			else
				"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --producer --allow-principal User:"$user" --topic "$topic" --resource-pattern-type PREFIXED --command-config "$producer_properties"
				
			fi
		else
			if [[ "$kafka_version" == "2.7.0" ]]; then
				"$kafka_acls_path" --authorizer-properties zookeeper.connect="$selected_zookeeper" --add --producer --allow-principal User:"$user" --topic "$topic" --resource-pattern-type PREFIXED
			else
				"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --producer --allow-principal User:"$user" --topic "$topic" --resource-pattern-type PREFIXED --command-config "$producer_properties"
			fi
		fi
	
	elif [[ "$there_is_csv" == "yes" ]]; then
		if { ( [[ ${csv_file_type} == "simple" ]] && [[ "$action" == "3" ]] ) || \
			 ( [[ ${csv_file_type} == "simple_only_topics" ]] && [[ "$action" == "2" ]] ) ;}
		then
			#if [[ "$kafka_version" == "2.7.0" ]]; then
				echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --producer --allow-principal User:$user --topic $topic" #>> "$sh_file"
				
			#else
			#	"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --producer --allow-principal User:"$user" --topic "$topic" --command-config "$producer_properties"
			#	
			#	log_mgmt "$ticket_num"
			#	
			#	log_zipper
			#fi
			
		elif { ( [[ ${csv_file_type} == "simple" ]] && [[ "$action" == "2" ]] ) || \
			   ( [[ ${csv_file_type} == "simple_only_topics" ]] && [[ "$action" == "3" ]] ) ;}
		then
			#if [[ "$kafka_version" == "2.7.0" ]]; then
				echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --producer --allow-principal User:$user --topic $topic --resource-pattern-type PREFIXED" #>> "$sh_file"
				
			#else
			#	"$kafka_acls_path" --bootstrap-server "$selected_broker" --add --producer --allow-principal User:"$user" --topic "$topic" --resource-pattern-type PREFIXED --command-config "$producer_properties"
			#	
			#	log_mgmt "$ticket_num"
			#	
			#	log_zipper
			#fi
			
		else
			local missing_vars=()
			[ -z "$user" ] && missing_vars=+"$user"
			[ -z "$topic" ] && missing_vars=+"$topic"
			
			check_error "$kafka_acls_path --add prefixed producer" "${missing_vars[@]}"
		fi
		
	fi
}
				  
function add_modify_rights_complex_csv() {
	
	case $operation in
	
	1) 
		echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --operation read --allow-principal User:$user --topic $topic_read"
		;;
		
	2)
		echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --operation describe --allow-principal User:$user --topic $topic_describe"
		;;
		
	3)
		echo "$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --operation write --allow-principal User:$user --topic $topic_write"
		;;
		
	esac
}

#declare actual_cmd=""

function add_modify_rights() {
	
	#sh_files+=("$sh_file")
	
	if [[ "$choose_param" == "2" ]]; then
		if [[ "$operation" != "describe_configs" ]]; then
			
			topics+="$topic"
			for topic in "${topics[@]}";
			#for topic in "${array[@]}"
			do
				modify_topic_rights_cmd="$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --operation $operation --allow-principal User:$user --topic $topic"
				echo "$modify_topic_rights_cmd" >> "$sh_file"
			done
			cat "$sh_file"
			
		elif [[ "$operation" == "describe_configs" ]]; then 
			describe_configs_cmd="$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --allow-principal User:$user --operation DescribeConfigs --topic $topic"
			echo "$describe_configs_cmd" >> "$sh_file"
			cat "$sh_file"
		
		
		else
			modify_topic_rights_cmd="$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --operation $operation --allow-principal User:$user --topic $topic"
			echo "$modify_topic_rights_cmd" >> "$sh_file"
			cat "$sh_file"
			
		fi
	
	elif [[ "$choose_param" == "1" ]]; then
		modify_consumer_gr_cmd="$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --allow-principal User:$user --group $consumer_group --operation $operation"
		echo "$modify_consumer_gr_cmd" >> "$sh_file"
		cat "$sh_file"
	
	elif [[ "$choose_param" == "3" ]]; then
		if [[ "$operation" == "idempotent_write" ]]; then
			modify_idempotent_write_cmd="$kafka_acls_path --authorizer-properties zookeeper.connect=$selected_zookeeper --add --allow-principal User:$user --operation IdempotentWrite --cluster"
			echo "$modify_idempotent_write_cmd" >> "$sh_file"
			cat "$sh_file"
		fi
		
	else
		printf '%s\n' \
		"${RED}HIBA: helytelen paramétert adtál meg! Lehetőségek: group, topic vagy cluster.${NC}" \
		"${YEL}Kérlek, újból add meg a szükséges adatokat.${NC}"
						
	fi
	
	#else
	#	local missing_vars=()
	#	[ -z "$operation" ] && missing_vars=+"$operation"
	#	[ -z "$user" ] && missing_vars=+"$user"
	#	[ -z "$topic" ] && missing_vars=+"$topic"
	#
	#	check_error "$kafka_acls_path --add modify rights" "${missing_vars[@]}"
	#fi
}

function add_modify_rights_no_csv_with_transactionalid() {
	
	#sh_files+=(""$sh_file"")
	
	if { [[ "$operation" == "write" ]] || [[ "$operation" == "read" ]] || [[ "$operation" == "describe" ]] ;}
	then
		modify_rights_transactional_id="$kafka_acls_path" --authorizer-properties zookeeper.connect="$selected_zookeeper":"$zk_port" --add --allow-principal User:"$user" --topic "$topic" --transactional-id "$transactional_id" --resource-pattern-type prefixed --operation "$operation"
		echo "$modify_rights_transactional_id"	
	else
		local missing_vars=()
		[ -z "$operation" ] && missing_vars=+"$operation"
		[ -z "$user" ] && missing_vars=+"$user"
		[ -z "$topic" ] && missing_vars=+"$topic"
		[ -z "$transactional_id" ] && missing_vars=+"$transactional_id"
	
		check_error "$kafka_acls_path --add modify rights with transactional-id" "${missing_vars[@]}"
	fi
}

function modify_retention_no_csv() {
	#"$kafka_configs_path" --alter --bootstrap-server "$selected_zookeeper":"$port" --entity-type topics --entity-name "$topic" --add-config retention.ms="$day_to_ms_retention"
	if [[ "$kafka_version" == "2.7.0" ]]; then
		modify_retention_cmd="$kafka_topics_path --zookeeper $selected_zookeeper:$zk_port --alter --topic $topic --config retention.ms=$day_to_ms_retention"
		echo "$modify_retention_cmd"
	else
		#"$kafka_topics_path" --bootstrap-server "$selected_broker" --alter --topic "$topic" --config retention.ms="$day_to_ms_retention" --command-config "$consumer_properties"
		modify_retention_cmd="$kafka_configs_path --bootstrap-server $selected_broker --entity-type topics --entity-name $topic --alter --add-config retention.ms=$day_to_ms_retention --command-config $consumer_properties"
		echo "$modify_retention_cmd"
	fi
		
	#else
	#	local missing_vars=()
	#	[ -z "$topic" ] && missing_vars=+"$topic"
	#	[ -z "$day_to_ms_retention" ] && missing_vars=+"$day_to_ms_retention"
	#
	#	check_error "$kafka_configs_path --alter topic retention" "${missing_vars[@]}"
	#fi
}

function modify_topic_partitions() {
	if [[ "$kafka_version" == "2.7.0" ]]; then
		"$kafka_topics_path" --alter --zookeeper "$selected_zookeeper":"$zk_port" --topic "$topic" --partitions "$partitions"
		
	else
		#"$kafka_topics_path" --alter --bootstrap-server "$selected_broker":"$bts_port" --topic "$topic" --partitions "$partitions"
		"$kafka_topics_path" --bootstrap-server "$selected_broker" --alter --topic "$topic" --partitions "$partitions" --command-config "$consumer_properties"
		
	fi
	
	#else
	#	local missing_vars=()
	#	[ -z "$topic" ] && missing_vars=+"$topic"
	#	[ -z "$partitions" ] && missing_vars=+"$partitions"
	#
	#	check_error "$kafka_topics_path --alter topic partitions" "${missing_vars[@]}"
	#fi
}

function reset_offsets() {
	if [[ "$offset_reset_type" == "to-earliest" ]]; then
		"$kafka_consumer_gr_path" --bootstrap-server "$selected_broker" --command-config "$consumer_properties" --group "$consumer_group" --topic "$topic" --reset-offsets --to-earliest --execute
	
	elif [[ "$offset_reset_type" == "to-latest" ]]; then
		"$kafka_consumer_gr_path" --bootstrap-server "$selected_broker" --command-config "$consumer_properties" --group "$consumer_group" --topic "$topic" --reset-offsets --to-latest --execute
		
	else
		offset_date_format
		
		"$kafka_consumer_gr_path" --bootstrap-server "$selected_broker" --command-config "$consumer_properties" --group "$consumer_group" --topic "$topic" --reset-offsets --to-datetime "$formatted_date" --execute
		
	fi
		
	#else
	#	local missing_vars=()
	#	[ -z "$topic" ] && missing_vars=+"$topic"
	#	[ -z "$consumer_group" ] && missing_vars=+"$consumer_group"
	#	
	#	check_error "$kafka_consumer_gr_path --reset-offsets" "${missing_vars[@]}"
	#	
	#fi

}

function delete_offsets {
	if [[ "$env" == "dv" ]]; then
		"$kafka_consumer_gr_path" --bootstrap-server "$selected_broker" --command-config "$consumer2_properties" --delete-offsets --group "$consumer_group" --topic "$topic"
		
	else
		"$kafka_consumer_gr_path" --bootstrap-server "$selected_broker" --command-config "$consumer_properties" --delete-offsets --group "$consumer_group" --topic "$topic"
		
	fi
	#else
	#	local missing_vars=()
	#	[ -z "$topic" ] && missing_vars=+"$topic"
	#	[ -z "$consumer_group" ] && missing_vars=+"$consumer_group"
	#	
	#	check_error "$kafka_consumer_gr_path --delete-offsets" "${missing_vars[@]}"
	#	
	#fi
			
}